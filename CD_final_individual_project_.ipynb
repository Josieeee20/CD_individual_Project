{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92dca078",
   "metadata": {},
   "source": [
    "# The intruduction of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e2a47",
   "metadata": {},
   "source": [
    "This project primarily investigates the (https://www.fmprc.gov.cn/mfa_eng/) Chinese Ministry of Foreign Affairs' public statements in December, examining the responses from China to relevant affairs of which countries or regions during the month, along with the expressed opinions and attitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5314a9df",
   "metadata": {},
   "source": [
    "#  Webscrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9a9ea1",
   "metadata": {},
   "source": [
    "## The introduction of the libraries\n",
    "- requests:\n",
    "Used for making HTTP requests to fetch data from web servers.\n",
    "BeautifulSoup:\n",
    "A library for parsing HTML and XML documents, commonly used in web scraping.\n",
    "- csv:\n",
    "A built-in module for reading and writing CSV files, simplifying handling tabular data.\n",
    "urllib.parse:\n",
    "Provides functions for parsing and manipulating URLs.\n",
    "- re:\n",
    "The regular expressions module for powerful string pattern matching and manipulation.\n",
    "- os:\n",
    "Interacts with the operating system, often used for file and directory manipulation.\n",
    "unicodedata:\n",
    "Provides access to the Unicode Character Database, useful for working with Unicode characters and strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e69170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import urllib.parse\n",
    "import re\n",
    "import os\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af06383",
   "metadata": {},
   "source": [
    "Firstly we collect the data from the website(https://www.fmprc.gov.cn/eng/xwfw_665399/s2510_665401/2511_665403/)and save it in a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84a319f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_save_news(url, output_file):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    news_list = soup.find(\"div\", class_=\"newsLst_mod\").find_all(\"li\")\n",
    "\n",
    "    data = []\n",
    "\n",
    "    base_url = \"https://www.fmprc.gov.cn/eng/xwfw_665399/s2510_665401/2511_665403/\"\n",
    "\n",
    "    for news_item in news_list:\n",
    "        title = news_item.find(\"a\").text.strip()\n",
    "        news_url_relative = news_item.find(\"a\")['href']\n",
    "\n",
    "        # Splice the relative path and the base URL to get the complete news link\n",
    "        news_url = urllib.parse.urljoin(base_url, news_url_relative)\n",
    "\n",
    "        # Get the content of the news content page\n",
    "        content_response = requests.get(news_url)\n",
    "        content_soup = BeautifulSoup(content_response.text, \"html.parser\")\n",
    "\n",
    "        # Get news content\n",
    "        content_paragraphs = content_soup.find(\"div\", class_=\"content\").find_all(\"p\")  \n",
    "        content_text = \" \".join([p.text.strip() for p in content_paragraphs])\n",
    "\n",
    "        # Add title and content to data list\n",
    "        data.append({\"Title\": title, \"Content\": content_text})\n",
    "\n",
    "    # Write data to CSV file\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        fieldnames = [\"Title\", \"Content\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Replace with actual URL and output filename\n",
    "url_to_scrape = \"https://www.fmprc.gov.cn/eng/xwfw_665399/s2510_665401/2511_665403/index.html\"\n",
    "output_csv_file = \"news_data.csv\"\n",
    "\n",
    "scrape_and_save_news(url_to_scrape, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f50d20",
   "metadata": {},
   "source": [
    "Due to the large amount of saved data, we only selected content with the title \"December.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a726d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_save_december_news(url, output_file):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    news_list = soup.find(\"div\", class_=\"newsLst_mod\").find_all(\"li\")\n",
    "\n",
    "    data = []\n",
    "\n",
    "    base_url = \"https://www.fmprc.gov.cn/eng/xwfw_665399/s2510_665401/2511_665403/\"\n",
    "\n",
    "    for news_item in news_list:\n",
    "        title = news_item.find(\"a\").text.strip()\n",
    "        \n",
    "        if \"December\" in title:\n",
    "            news_url_relative = news_item.find(\"a\")['href']\n",
    "            \n",
    "            # Splice the relative path and the base URL to get the complete news link\n",
    "            news_url = urllib.parse.urljoin(base_url, news_url_relative)\n",
    "\n",
    "            # Get the content of the news content page\n",
    "            content_response = requests.get(news_url)\n",
    "            content_soup = BeautifulSoup(content_response.text, \"html.parser\")\n",
    "            content_paragraphs = content_soup.find(\"div\", class_=\"content\").find_all(\"p\") \n",
    "            content_text = \" \".join([p.text.strip() for p in content_paragraphs])\n",
    "\n",
    "            # Add title and content to data list\n",
    "            data.append({\"Title\": title, \"Content\": content_text})\n",
    "\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        fieldnames = [\"Title\", \"Content\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "url_to_scrape = \"https://www.fmprc.gov.cn/eng/xwfw_665399/s2510_665401/2511_665403/index.html\"\n",
    "output_csv_file = \"december_news_data.csv\"\n",
    "\n",
    "scrape_and_save_december_news(url_to_scrape, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec802c64",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c70a303",
   "metadata": {},
   "source": [
    "Clean up the text, retaining only normal punctuation and textual content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9c78ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z.,!? ]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def clean_and_save_to_csv(input_csv_file, output_csv_file):\n",
    "    data = []\n",
    "\n",
    "    with open(input_csv_file, 'r', encoding='utf-8') as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            title = row[\"Title\"]\n",
    "            content = row[\"Content\"]\n",
    "\n",
    "            cleaned_content_text = clean_text(content)\n",
    "\n",
    "            data.append({\"Title\": title, \"Content\": cleaned_content_text})\n",
    "\n",
    "    with open(output_csv_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        fieldnames = [\"Title\", \"Content\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "input_csv_file = \"december_news_data.csv\"\n",
    "output_csv_file = \"cleaned_data.csv\"\n",
    "\n",
    "clean_and_save_to_csv(input_csv_file, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3823e847",
   "metadata": {},
   "source": [
    "Split the text content into multiple TXT files and rename them in the format of \"txt_01.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "173eaaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_to_txt(title, content, output_folder, file_count):\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    filename = f\"txt_{file_count:02d}.txt\"\n",
    "\n",
    "    with open(os.path.join(output_folder, filename), 'w', encoding='utf-8') as txt_file:\n",
    "        txt_file.write(f\"Title: {title}\\n\\n\")\n",
    "        txt_file.write(f\"Content:\\n{content}\")\n",
    "\n",
    "def split_csv_to_txt(input_csv_file, output_folder):\n",
    "    with open(input_csv_file, 'r', encoding='utf-8') as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for idx, row in enumerate(reader, start=1):\n",
    "            title = row[\"Title\"]\n",
    "            content = row[\"Content\"]\n",
    "\n",
    "            cleaned_content_text = clean_text(content)\n",
    "\n",
    "            save_to_txt(title, cleaned_content_text, output_folder, idx)\n",
    "\n",
    "input_csv_file = \"cleaned_data.csv\"\n",
    "output_folder = \"output_txt_files\"\n",
    "\n",
    "split_csv_to_txt(input_csv_file, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52264088",
   "metadata": {},
   "source": [
    "# Install Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a739c55d",
   "metadata": {},
   "source": [
    "- Install and Import Libraries: The code installs spaCy and Plotly libraries using %pip install and imports necessary packages.\n",
    "\n",
    "- Install English Language Model: It downloads and installs the English language model for spaCy using !python -m spacy download en_core_web_sm.\n",
    "\n",
    "- Load spaCy Model: It loads the English language model into spaCy using nlp = spacy.load(\"en_core_web_sm\").\n",
    "\n",
    "- Import spaCy Visualizer: The code imports the spaCy visualizer displacy for later use in visualizing text annotations.\n",
    "\n",
    "- Import Other Libraries: It imports additional libraries such as os for file handling, pandas for data manipulation, and Plotly for graphing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e8f6149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spaCy in /Users/josiechen/anaconda3/lib/python3.11/site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spaCy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spaCy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spaCy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spaCy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spaCy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spaCy) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spaCy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spaCy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spaCy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spaCy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spaCy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spaCy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spaCy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spaCy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spaCy) (2.5.2)\n",
      "Requirement already satisfied: jinja2 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spaCy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spaCy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spaCy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spaCy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spaCy) (1.24.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spaCy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spaCy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spaCy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spaCy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from jinja2->spaCy) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: plotly in /Users/josiechen/anaconda3/lib/python3.11/site-packages (5.9.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from plotly) (8.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nbformat in /Users/josiechen/anaconda3/lib/python3.11/site-packages (5.9.2)\n",
      "Requirement already satisfied: fastjsonschema in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from nbformat) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from nbformat) (4.17.3)\n",
      "Requirement already satisfied: jupyter-core in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from nbformat) (5.3.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from nbformat) (5.7.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from jsonschema>=2.6->nbformat) (0.18.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from jupyter-core->nbformat) (2.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install and import spacy and plotly.\n",
    "%pip install spaCy\n",
    "%pip install plotly\n",
    "%pip install nbformat --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdb3d6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.2)\n",
      "Requirement already satisfied: jinja2 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/josiechen/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Install English language model\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Import os to upload documents and metadata\n",
    "import os\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load spaCy visualizer\n",
    "from spacy import displacy\n",
    "\n",
    "# Import pandas DataFrame packages\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# Import graphing package\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed1c1ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists for file names and contents\n",
    "texts = []\n",
    "file_names = []\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for _file_name in os.listdir('cleaned_txt_files'):\n",
    "# Look for only text files\n",
    "    if _file_name.endswith('.txt'):\n",
    "    # Append contents of each text file to text list\n",
    "        texts.append(open('cleaned_txt_files' + '/' + _file_name, 'r', encoding='utf-8').read())\n",
    "        # Append name of each file to file name list\n",
    "        file_names.append(_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "756bed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary object associating each file name with its text\n",
    "d = {'Filename':file_names,'Text':texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bf62adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn dictionary into a dataframe\n",
    "News_df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb8176d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>txt_11.txt</td>\n",
       "      <td>We noted that General Secretary and President ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>txt_05.txt</td>\n",
       "      <td>CCTV This year marks the tenth anniversary of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>txt_04.txt</td>\n",
       "      <td>China News Service Premier Li Qiang attended t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>txt_10.txt</td>\n",
       "      <td>At the invitation of Premier of the State Coun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>txt_06.txt</td>\n",
       "      <td>The fourth LancangMekong Cooperation LMC Leade...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Filename                                               Text\n",
       "0  txt_11.txt  We noted that General Secretary and President ...\n",
       "1  txt_05.txt  CCTV This year marks the tenth anniversary of ...\n",
       "2  txt_04.txt  China News Service Premier Li Qiang attended t...\n",
       "3  txt_10.txt  At the invitation of Premier of the State Coun...\n",
       "4  txt_06.txt  The fourth LancangMekong Cooperation LMC Leade..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove extra spaces from papers\n",
    "News_df['Text'] = News_df['Text'].str.replace('\\s+', ' ', regex=True).str.strip()\n",
    "News_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2dc3a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper_ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Spokesman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>txt_01</td>\n",
       "      <td>Foreign Ministry Spokesperson Mao Nings Regula...</td>\n",
       "      <td>29-Dec-23</td>\n",
       "      <td>Mao Ning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>txt_02</td>\n",
       "      <td>Foreign Ministry Spokesperson Mao Nings Regula...</td>\n",
       "      <td>28-Dec-23</td>\n",
       "      <td>Mao Ning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>txt_03</td>\n",
       "      <td>Foreign Ministry Spokesperson Mao Nings Regula...</td>\n",
       "      <td>27-Dec-23</td>\n",
       "      <td>Mao Ning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>txt_04</td>\n",
       "      <td>Foreign Ministry Spokesperson Mao Nings Regula...</td>\n",
       "      <td>26-Dec-23</td>\n",
       "      <td>Mao Ning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>txt_05</td>\n",
       "      <td>Foreign Ministry Spokesperson Mao Nings Regula...</td>\n",
       "      <td>25-Dec-23</td>\n",
       "      <td>Mao Ning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Paper_ID                                              Title       Date  \\\n",
       "0   txt_01  Foreign Ministry Spokesperson Mao Nings Regula...  29-Dec-23   \n",
       "1   txt_02  Foreign Ministry Spokesperson Mao Nings Regula...  28-Dec-23   \n",
       "2   txt_03  Foreign Ministry Spokesperson Mao Nings Regula...  27-Dec-23   \n",
       "3   txt_04  Foreign Ministry Spokesperson Mao Nings Regula...  26-Dec-23   \n",
       "4   txt_05  Foreign Ministry Spokesperson Mao Nings Regula...  25-Dec-23   \n",
       "\n",
       "  Spokesman  \n",
       "0  Mao Ning  \n",
       "1  Mao Ning  \n",
       "2  Mao Ning  \n",
       "3  Mao Ning  \n",
       "4  Mao Ning  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load metadata.\n",
    "metadata_df = pd.read_csv('metafile.csv')\n",
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab513811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove .txt from title of each paper\n",
    "News_df['Filename'] = News_df['Filename'].str.replace('.txt', '', regex=True)\n",
    "\n",
    "# Rename column from paper ID to Title\n",
    "metadata_df.rename(columns={\"Paper_ID\": \"Filename\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fe17e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge metadata and papers into new DataFrame\n",
    "# Will only keep rows where both essay and metadata are present\n",
    "final_News_df = metadata_df.merge(News_df,on='Filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ac6e295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Spokesman</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>txt_01</td>\n",
       "      <td>Foreign Ministry Spokesperson Mao Nings Regula...</td>\n",
       "      <td>29-Dec-23</td>\n",
       "      <td>Mao Ning</td>\n",
       "      <td>CNR This year, President Xi Jinping visited As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>txt_02</td>\n",
       "      <td>Foreign Ministry Spokesperson Mao Nings Regula...</td>\n",
       "      <td>28-Dec-23</td>\n",
       "      <td>Mao Ning</td>\n",
       "      <td>AFP Chinas Embassy in Myanmar today reminded a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>txt_03</td>\n",
       "      <td>Foreign Ministry Spokesperson Mao Nings Regula...</td>\n",
       "      <td>27-Dec-23</td>\n",
       "      <td>Mao Ning</td>\n",
       "      <td>CCTV This year marks the th anniversary of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>txt_04</td>\n",
       "      <td>Foreign Ministry Spokesperson Mao Nings Regula...</td>\n",
       "      <td>26-Dec-23</td>\n",
       "      <td>Mao Ning</td>\n",
       "      <td>China News Service Premier Li Qiang attended t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>txt_05</td>\n",
       "      <td>Foreign Ministry Spokesperson Mao Nings Regula...</td>\n",
       "      <td>25-Dec-23</td>\n",
       "      <td>Mao Ning</td>\n",
       "      <td>CCTV This year marks the tenth anniversary of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Filename                                              Title       Date  \\\n",
       "0   txt_01  Foreign Ministry Spokesperson Mao Nings Regula...  29-Dec-23   \n",
       "1   txt_02  Foreign Ministry Spokesperson Mao Nings Regula...  28-Dec-23   \n",
       "2   txt_03  Foreign Ministry Spokesperson Mao Nings Regula...  27-Dec-23   \n",
       "3   txt_04  Foreign Ministry Spokesperson Mao Nings Regula...  26-Dec-23   \n",
       "4   txt_05  Foreign Ministry Spokesperson Mao Nings Regula...  25-Dec-23   \n",
       "\n",
       "  Spokesman                                               Text  \n",
       "0  Mao Ning  CNR This year, President Xi Jinping visited As...  \n",
       "1  Mao Ning  AFP Chinas Embassy in Myanmar today reminded a...  \n",
       "2  Mao Ning  CCTV This year marks the th anniversary of the...  \n",
       "3  Mao Ning  China News Service Premier Li Qiang attended t...  \n",
       "4  Mao Ning  CCTV This year marks the tenth anniversary of ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print DataFrame\n",
    "final_News_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fdfb9e",
   "metadata": {},
   "source": [
    "# Text Enrichment with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5c36ea",
   "metadata": {},
   "source": [
    "## Creating Doc Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e9f6e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Load nlp pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Check what functions it performs\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59b496ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that runs the nlp pipeline on any given input text\n",
    "def process_text(text):\n",
    "    return nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "608a7cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the \"Text\" column, so that the nlp pipeline is called on each student essay\n",
    "final_News_df['Doc'] = final_News_df['Text'].apply(process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5bf97c",
   "metadata": {},
   "source": [
    "# Text Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db51e32",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65590bb",
   "metadata": {},
   "source": [
    "A critical first step spaCy performs is tokenization, or the segmentation of strings into individual words and punctuation markers. Tokenization enables spaCy to parse the grammatical structures of a text and identify characteristics of each word-like part-of-speech.\n",
    "\n",
    "To retrieve a tokenized version of each text in the DataFrame, we’ll write a function that iterates through any given Doc object and returns all functions found within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "381d5b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to retrieve tokens from a doc object\n",
    "def get_token(doc):\n",
    "    return [(token.text) for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2549ba81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Spokesman</th>\n",
       "      <th>Text</th>\n",
       "      <th>Doc</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>txt_01</td>\n",
       "      <td>Foreign Ministry Spokesperson Mao Nings Regula...</td>\n",
       "      <td>29-Dec-23</td>\n",
       "      <td>Mao Ning</td>\n",
       "      <td>CNR This year, President Xi Jinping visited As...</td>\n",
       "      <td>(CNR, This, year, ,, President, Xi, Jinping, v...</td>\n",
       "      <td>[CNR, This, year, ,, President, Xi, Jinping, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>txt_02</td>\n",
       "      <td>Foreign Ministry Spokesperson Mao Nings Regula...</td>\n",
       "      <td>28-Dec-23</td>\n",
       "      <td>Mao Ning</td>\n",
       "      <td>AFP Chinas Embassy in Myanmar today reminded a...</td>\n",
       "      <td>(AFP, Chinas, Embassy, in, Myanmar, today, rem...</td>\n",
       "      <td>[AFP, Chinas, Embassy, in, Myanmar, today, rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>txt_03</td>\n",
       "      <td>Foreign Ministry Spokesperson Mao Nings Regula...</td>\n",
       "      <td>27-Dec-23</td>\n",
       "      <td>Mao Ning</td>\n",
       "      <td>CCTV This year marks the th anniversary of the...</td>\n",
       "      <td>(CCTV, This, year, marks, the, th, anniversary...</td>\n",
       "      <td>[CCTV, This, year, marks, the, th, anniversary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>txt_04</td>\n",
       "      <td>Foreign Ministry Spokesperson Mao Nings Regula...</td>\n",
       "      <td>26-Dec-23</td>\n",
       "      <td>Mao Ning</td>\n",
       "      <td>China News Service Premier Li Qiang attended t...</td>\n",
       "      <td>(China, News, Service, Premier, Li, Qiang, att...</td>\n",
       "      <td>[China, News, Service, Premier, Li, Qiang, att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>txt_05</td>\n",
       "      <td>Foreign Ministry Spokesperson Mao Nings Regula...</td>\n",
       "      <td>25-Dec-23</td>\n",
       "      <td>Mao Ning</td>\n",
       "      <td>CCTV This year marks the tenth anniversary of ...</td>\n",
       "      <td>(CCTV, This, year, marks, the, tenth, annivers...</td>\n",
       "      <td>[CCTV, This, year, marks, the, tenth, annivers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Filename                                              Title       Date  \\\n",
       "0   txt_01  Foreign Ministry Spokesperson Mao Nings Regula...  29-Dec-23   \n",
       "1   txt_02  Foreign Ministry Spokesperson Mao Nings Regula...  28-Dec-23   \n",
       "2   txt_03  Foreign Ministry Spokesperson Mao Nings Regula...  27-Dec-23   \n",
       "3   txt_04  Foreign Ministry Spokesperson Mao Nings Regula...  26-Dec-23   \n",
       "4   txt_05  Foreign Ministry Spokesperson Mao Nings Regula...  25-Dec-23   \n",
       "\n",
       "  Spokesman                                               Text  \\\n",
       "0  Mao Ning  CNR This year, President Xi Jinping visited As...   \n",
       "1  Mao Ning  AFP Chinas Embassy in Myanmar today reminded a...   \n",
       "2  Mao Ning  CCTV This year marks the th anniversary of the...   \n",
       "3  Mao Ning  China News Service Premier Li Qiang attended t...   \n",
       "4  Mao Ning  CCTV This year marks the tenth anniversary of ...   \n",
       "\n",
       "                                                 Doc  \\\n",
       "0  (CNR, This, year, ,, President, Xi, Jinping, v...   \n",
       "1  (AFP, Chinas, Embassy, in, Myanmar, today, rem...   \n",
       "2  (CCTV, This, year, marks, the, th, anniversary...   \n",
       "3  (China, News, Service, Premier, Li, Qiang, att...   \n",
       "4  (CCTV, This, year, marks, the, tenth, annivers...   \n",
       "\n",
       "                                              Tokens  \n",
       "0  [CNR, This, year, ,, President, Xi, Jinping, v...  \n",
       "1  [AFP, Chinas, Embassy, in, Myanmar, today, rem...  \n",
       "2  [CCTV, This, year, marks, the, th, anniversary...  \n",
       "3  [China, News, Service, Premier, Li, Qiang, att...  \n",
       "4  [CCTV, This, year, marks, the, tenth, annivers...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the token retrieval function on the doc objects in the dataframe\n",
    "final_News_df['Tokens'] = final_News_df['Doc'].apply(get_token)\n",
    "final_News_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e80c31b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNR This year, President Xi Jinping visited As...</td>\n",
       "      <td>[CNR, This, year, ,, President, Xi, Jinping, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFP Chinas Embassy in Myanmar today reminded a...</td>\n",
       "      <td>[AFP, Chinas, Embassy, in, Myanmar, today, rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCTV This year marks the th anniversary of the...</td>\n",
       "      <td>[CCTV, This, year, marks, the, th, anniversary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>China News Service Premier Li Qiang attended t...</td>\n",
       "      <td>[China, News, Service, Premier, Li, Qiang, att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCTV This year marks the tenth anniversary of ...</td>\n",
       "      <td>[CCTV, This, year, marks, the, tenth, annivers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  CNR This year, President Xi Jinping visited As...   \n",
       "1  AFP Chinas Embassy in Myanmar today reminded a...   \n",
       "2  CCTV This year marks the th anniversary of the...   \n",
       "3  China News Service Premier Li Qiang attended t...   \n",
       "4  CCTV This year marks the tenth anniversary of ...   \n",
       "\n",
       "                                              Tokens  \n",
       "0  [CNR, This, year, ,, President, Xi, Jinping, v...  \n",
       "1  [AFP, Chinas, Embassy, in, Myanmar, today, rem...  \n",
       "2  [CCTV, This, year, marks, the, th, anniversary...  \n",
       "3  [China, News, Service, Premier, Li, Qiang, att...  \n",
       "4  [CCTV, This, year, marks, the, tenth, annivers...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = final_News_df[['Text', 'Tokens']].copy()\n",
    "tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c0dc9e",
   "metadata": {},
   "source": [
    "# ❗️❗️Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d0d692",
   "metadata": {},
   "source": [
    "# What are the most frequent words used in the speech?Does this express China's stance on international affairs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52e1dcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('china', 629), ('cooperation', 280), ('countries', 278), ('chinas', 241), ('two', 172), ('international', 167), ('president', 162), ('development', 161), ('us', 155), ('chinese', 153)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/josiechen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Assuming 'Tokens' column contains a list of tokens for each document\n",
    "all_tokens = [token for tokens_list in final_News_df['Tokens'] for token in tokens_list]\n",
    "\n",
    "# Remove stop words and punctuation\n",
    "filtered_tokens = [token.lower() for token in all_tokens if token.lower() not in stop_words and token not in string.punctuation]\n",
    "\n",
    "# Analyze word frequency\n",
    "word_frequency = Counter(filtered_tokens)\n",
    "\n",
    "# Get the top N words\n",
    "N = 10  # Replace with the desired number of top words\n",
    "top_words = word_frequency.most_common(N)\n",
    "\n",
    "# Display the top words\n",
    "print(top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7727c856",
   "metadata": {},
   "source": [
    "## conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e35ac44",
   "metadata": {},
   "source": [
    "We can see that besides 'China,' the most frequently mentioned terms are 'cooperation' and 'development.' This indicates China's desire for friendly relations with other countries, the elimination of prejudice and discrimination, while also emphasizing its own stance and safeguarding the interests of the Chinese people."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5045a501",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d861a89f",
   "metadata": {},
   "source": [
    "Another process performed by spaCy is lemmatization, or the retrieval of the dictionary root word of each word (for example “brighten” for “brightening”). We’ll perform a similar set of steps to those above to create a function to call the lemmas from the Doc object, then apply it to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f692c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to retrieve lemmas from a doc object\n",
    "def get_lemma(doc):\n",
    "    return [(token.lemma_) for token in doc]\n",
    "\n",
    "# Run the lemma retrieval function on the doc objects in the dataframe\n",
    "final_News_df['Lemmas'] = final_News_df['Doc'].apply(get_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec9f3d",
   "metadata": {},
   "source": [
    "## Text Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaf9d18",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9dbf4",
   "metadata": {},
   "source": [
    "spaCy facilitates two levels of part-of-speech tagging: coarse-grained tagging, which predicts the simple universal part-of-speech of each token in a text (such as noun, verb, adjective, adverb), and detailed tagging, which uses a larger, more fine-grained set of part-of-speech tags (for example 3rd person singular present verb). The part-of-speech tags used are determined by the English language model we use. In this case, we’re using the small English model, and you can explore the differences between the models on spaCy’s website.\n",
    "\n",
    "We can call the part-of-speech tags in the same way as the lemmas. Create a function to extract them from any given Doc object and apply the function to each Doc object in the DataFrame. The function we’ll create will extract both the coarse- and fine-grained part-of-speech for each token (token.pos_ and token.tag_, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da81862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to retrieve lemmas from a doc object\n",
    "def get_pos(doc):\n",
    "    #Return the coarse- and fine-grained part of speech text for each token in the doc\n",
    "    return [(token.pos_, token.tag_) for token in doc]\n",
    "\n",
    "# Define a function to retrieve parts of speech from a doc object\n",
    "final_News_df['POS'] = final_News_df['Doc'].apply(get_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7e57f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract proper nouns from Doc object\n",
    "def extract_proper_nouns(doc):\n",
    "    return [token.text for token in doc if token.pos_ == 'PROPN']\n",
    "\n",
    "# Apply function to Doc column and store resulting proper nouns in new column\n",
    "final_News_df['Proper_Nouns'] = final_News_df['Doc'].apply(extract_proper_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87fdd4b",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08baa7e2",
   "metadata": {},
   "source": [
    "spaCy can tag named entities in the text, such as names, dates, organizations, and locations. Call the full list of named entities and their descriptions using this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all NE labels and assign to variable\n",
    "labels = nlp.get_pipe(\"ner\").labels\n",
    "\n",
    "# Print each label and its description\n",
    "for label in labels:\n",
    "    print(label + ' : ' + spacy.explain(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5248738c",
   "metadata": {},
   "source": [
    "Let's check the named entity recognition of the full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4955e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract named entities from doc objects\n",
    "def extract_named_entities(doc):\n",
    "    return [ent.label_ for ent in doc.ents]\n",
    "\n",
    "# Apply function to Doc column and store resulting named entities in new column\n",
    "final_News_df['Named_Entities'] = final_News_df['Doc'].apply(extract_named_entities)\n",
    "final_News_df['Named_Entities']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217e1666",
   "metadata": {},
   "source": [
    "We can add another column with the words and phrases identified as named entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da6919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract text tagged with named entities from doc objects\n",
    "def extract_named_entities(doc):\n",
    "    return [ent for ent in doc.ents]\n",
    "\n",
    "# Apply function to Doc column and store resulting text in new column\n",
    "final_News_df['NE_Words'] = final_News_df['Doc'].apply(extract_named_entities)\n",
    "final_News_df['NE_Words']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204bb338",
   "metadata": {},
   "source": [
    "Let’s visualize the words and their named entity tags in a single text. Call the first text’s Doc object and use displacy.render to visualize the text with the named entities highlighted and tagged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378478ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first Doc object\n",
    "doc = final_News_df['Doc'][1]\n",
    "\n",
    "# Visualize named entity tagging in a single paper\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dda258",
   "metadata": {},
   "source": [
    "# Download Enriched Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b157d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame as csv (in Google Drive)\n",
    "# Use this step only to save  csv to your computer's working directory\n",
    "final_News_df.to_csv('MICUSP_papers_with_spaCy_tags.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb2610d",
   "metadata": {},
   "source": [
    "# Analysis of Linguistic Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7527d0",
   "metadata": {},
   "source": [
    "## Part of Speech Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41dee4",
   "metadata": {},
   "source": [
    "spaCy counts the number of each part-of-speech tag that appears in each document (for example the number of times the NOUN tag appears in a document). This is called using doc.count_by(spacy.attrs.POS). Here’s how it works on a single sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c547fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create doc object from single sentence\n",
    "doc = nlp(\"This is 'an' example? sentence\")\n",
    "\n",
    "# Print counts of each part of speech in sentence\n",
    "print(doc.count_by(spacy.attrs.POS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea2d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store dictionary with indexes and POS counts in a variable\n",
    "num_pos = doc.count_by(spacy.attrs.POS)\n",
    "\n",
    "dictionary = {}\n",
    "\n",
    "# Create a new dictionary which replaces the index of each part of speech for its label (NOUN, VERB, ADJECTIVE)\n",
    "for k,v in sorted(num_pos.items()):\n",
    "  dictionary[doc.vocab[k].text] = v\n",
    "\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25a2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrame for analysis purposes\n",
    "pos_analysis_df = final_News_df[['Filename','Spokesman', 'Doc']]\n",
    "\n",
    "# Create list to store each dictionary\n",
    "num_list = []\n",
    "\n",
    "# Define a function to get part of speech tags and counts and append them to a new dictionary\n",
    "def get_pos_tags(doc):\n",
    "    dictionary = {}\n",
    "    num_pos = doc.count_by(spacy.attrs.POS)\n",
    "    for k,v in sorted(num_pos.items()):\n",
    "        dictionary[doc.vocab[k].text] = v\n",
    "    num_list.append(dictionary)\n",
    "\n",
    "# Apply function to each doc object in DataFrame\n",
    "pos_analysis_df.loc['C_POS'] = pos_analysis_df['Doc'].apply(get_pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98479122",
   "metadata": {},
   "source": [
    "# ❗️❗️Research question:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5497920e",
   "metadata": {},
   "source": [
    "## Do spokespersons Mao Ning and Wang Wenbing use certain parts of speech more frequently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f09b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe with part of speech counts\n",
    "pos_counts = pd.DataFrame(num_list)\n",
    "columns = list(pos_counts.columns)\n",
    "\n",
    "# Add discipline of each paper as new column to dataframe\n",
    "idx = 0\n",
    "new_col = pos_analysis_df['Spokesman']\n",
    "pos_counts.insert(loc=idx, column='Spokesman', value=new_col)\n",
    "\n",
    "pos_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average part of speech counts used in papers of each discipline\n",
    "average_pos_df = pos_counts.groupby(['Spokesman']).mean()\n",
    "\n",
    "# Round calculations to the nearest whole number\n",
    "average_pos_df = average_pos_df.round(0)\n",
    "\n",
    "# Reset index to improve DataFrame readability\n",
    "average_pos_df = average_pos_df.reset_index()\n",
    "\n",
    "# Show dataframe\n",
    "average_pos_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b258c",
   "metadata": {},
   "source": [
    "In the speech statistics for December, Wang Wenbing surpasses Mao Ning in various metrics. Even with an equal number of speeches, it is evident that Wang Wenbing utilizes a more diverse range of vocabulary and covers a broader range of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf82b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use plotly to plot proper noun use per genre\n",
    "fig = px.bar(average_pos_df, x=\"Spokesman\", y=[\"ADJ\", 'VERB', \"NUM\",\"ADV\"], title=\"Average Part-of-Speech Use in Papers Written by Biology and English Students\", barmode='group')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781ae01",
   "metadata": {},
   "source": [
    "## conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92267945",
   "metadata": {},
   "source": [
    "According to data visualization, overall, there isn't a significant difference in the language preferences of the two spokespersons. Wang Wenbing, relatively speaking, tends to use more nouns and verbs. This contributes to more persuasive and robust speeches, which may be related to gender differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b468c2",
   "metadata": {},
   "source": [
    "# Analysis of GPE Named Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e17807",
   "metadata": {},
   "source": [
    "# ❗️❗️Research question:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39c2c4",
   "metadata": {},
   "source": [
    "## In the December press briefings of the Ministry of Foreign Affairs, which country or region was mentioned the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe46dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the index with the desired document's index\n",
    "doc_to_analyze = final_News_df['Doc'][0]  # Replace 0 with the desired index\n",
    "\n",
    "# Extract only GPE entities\n",
    "gpe_entities = [ent.text for ent in doc_to_analyze.ents if ent.label_ == 'GPE']\n",
    "\n",
    "# Count the frequency of each GPE entity\n",
    "gpe_entity_counts = {}\n",
    "for entity in gpe_entities:\n",
    "    gpe_entity_counts[entity] = gpe_entity_counts.get(entity, 0) + 1\n",
    "\n",
    "# Print the frequency of each GPE entity\n",
    "for entity, count in gpe_entity_counts.items():\n",
    "    print(f'{entity}: {count}')\n",
    "\n",
    "# Visualize named entities of type GPE\n",
    "gpe_doc = spacy.tokens.Doc(doc_to_analyze.vocab, words=gpe_entities)\n",
    "displacy.render(gpe_doc, style='ent', jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f1d72",
   "metadata": {},
   "source": [
    "## conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05e639f",
   "metadata": {},
   "source": [
    "According to the statistics, it can be observed that several countries and regions were mentioned, with China and Myanmar being mentioned the most. This is related to the recent international situation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7b23c7",
   "metadata": {},
   "source": [
    "# In conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6404f7e",
   "metadata": {},
   "source": [
    "In summary, the analysis of speeches from the two spokespersons reveals a common emphasis on terms such as 'cooperation' and 'development,' reflecting China's commitment to fostering friendly relations, eliminating prejudice, and safeguarding its people's interests. Data visualization indicates that, overall, there isn't a significant difference in the language preferences of the two spokespersons, but Wang Wenbing tends to employ more nouns and verbs, contributing to more persuasive speeches, possibly influenced by gender differences.\n",
    "\n",
    "Furthermore, the statistical findings highlight the frequent mentions of China and Myanmar, suggesting a correlation with recent international developments. This comprehensive overview sheds light on the spokespersons' communication strategies and the geopolitical context influencing their discourse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
